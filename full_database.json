[
   {
    "name": "GPT-3 175B (DaVinci?)",
    "series": "GPT-3",
    "organization_name": "OpenAI",
    "publication_name": "Language Models are Few-Shot Learners (Brown et al., 2020)",
    "publication_link": "https://arxiv.org/abs/2005.14165",
    "publication_date": "2020-05-20",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 175,
    "model_availability": "commercial (API)",
    "model_link": "https://beta.openai.com/",
    "training_compute": 3640,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, DeepSpeed",
    "training_provider": "Microsoft",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": 218,
    "dataset_training_tokens": 300,
    "dataset_training_size": 428,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "GPT-3 Curie",
    "series": "GPT-3",
    "organization_name": "OpenAI",
    "publication_name": "Language Models are Few-Shot Learners (Brown et al., 2020)",
    "publication_link": "https://arxiv.org/abs/2005.14165",
    "publication_date": "2020-05-20",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 12.9,
    "model_availability": "commercial (API)",
    "model_link": "https://beta.openai.com/",
    "training_compute": 268,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, DeepSpeed",
    "training_provider": "Microsoft",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": 218,
    "dataset_training_tokens": 300,
    "dataset_training_size": 428,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "GPT-3 Babbage",
    "series": "GPT-3",
    "organization_name": "OpenAI",
    "publication_name": "Language Models are Few-Shot Learners (Brown et al., 2020)",
    "publication_link": "https://arxiv.org/abs/2005.14165",
    "publication_date": "2020-05-20",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 6.7,
    "model_availability": "commercial (API)",
    "model_link": "https://beta.openai.com/",
    "training_compute": 139,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, DeepSpeed",
    "training_provider": "Microsoft",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": 218,
    "dataset_training_tokens": 300,
    "dataset_training_size": 428,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "GPT-3 Ada",
    "series": "GPT-3",
    "organization_name": "OpenAI",
    "publication_name": "Language Models are Few-Shot Learners (Brown et al., 2020)",
    "publication_link": "https://arxiv.org/abs/2005.14165",
    "publication_date": "2020-05-20",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 2.7,
    "model_availability": "commercial (API)",
    "model_link": "https://beta.openai.com/",
    "training_compute": 55,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, DeepSpeed",
    "training_provider": "Microsoft",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": 218,
    "dataset_training_tokens": 300,
    "dataset_training_size": 428,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "PAGnol-XL",
    "series": "PAGnol",
    "organization_name": "LightOn",
    "publication_name": "PAGnol: An Extra-Large French Generative Model (Launay et al., 2021)",
    "publication_link": "https://lair.lighton.ai/pagnol/",
    "publication_date": "2021-05-04",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 1.5,
    "model_availability": "public",
    "model_link": "https://github.com/lightonai/lairgpt",
    "training_compute": 3,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, FairSeq",
    "training_provider": "Jean Zay",
    "training_hardware": "V100",
    "dataset": "CCNet",
    "dataset_unique_tokens": 32,
    "dataset_training_tokens": 32,
    "dataset_training_size": 133,
    "dataset_availability": "reproducible",
    "dataset_link": "https://github.com/facebookresearch/cc_net",
    "dataset_content": "French"
  },
  {
    "name": "PanGu-α",
    "series": "PanGu",
    "organization_name": "Huawei",
    "publication_name": "PanGu-α: Large-Scale Autoregressive Pretrained Chinese Language Models with Auto-Parallel Computation (Zeng et al., 2021)",
    "publication_link": "https://arxiv.org/abs/2104.12369",
    "publication_date": "2021-04-26",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 207,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 583,
    "training_code_availability": "public",
    "training_code_link": "https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-模型下载",
    "training_framework": "MindSpore",
    "training_provider": "Huawei",
    "training_hardware": "Ascend 910",
    "dataset": "custom",
    "dataset_unique_tokens": 258,
    "dataset_training_tokens": 42,
    "dataset_training_size": 1100,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "Chinese"
  },
  {
    "name": "PanGu-α 13B",
    "series": "PanGu",
    "organization_name": "Huawei",
    "publication_name": "PanGu-α: Large-Scale Autoregressive Pretrained Chinese Language Models with Auto-Parallel Computation (Zeng et al., 2021)",
    "publication_link": "https://arxiv.org/abs/2104.12369",
    "publication_date": "2021-04-26",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 13.1,
    "model_availability": "public",
    "model_link": "https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-模型下载",
    "training_compute": 63,
    "training_code_availability": "public",
    "training_code_link": "https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-模型下载",
    "training_framework": "MindSpore",
    "training_provider": "Huawei",
    "training_hardware": "Ascend 910",
    "dataset": "custom",
    "dataset_unique_tokens": 27,
    "dataset_training_tokens": 70,
    "dataset_training_size": 100,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "Chinese"
  },
  {
    "name": "PanGu-α 2.6B",
    "series": "PanGu",
    "organization_name": "Huawei",
    "publication_name": "PanGu-α: Large-Scale Autoregressive Pretrained Chinese Language Models with Auto-Parallel Computation (Zeng et al., 2021)",
    "publication_link": "https://arxiv.org/abs/2104.12369",
    "publication_date": "2021-04-26",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 2.6,
    "model_availability": "public",
    "model_link": "https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-模型下载",
    "training_compute": 13,
    "training_code_availability": "public",
    "training_code_link": "https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha#user-content-模型下载",
    "training_framework": "MindSpore",
    "training_provider": "Huawei",
    "training_hardware": "Ascend 910",
    "dataset": "custom",
    "dataset_unique_tokens": 27,
    "dataset_training_tokens": 70,
    "dataset_training_size": 100,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "Chinese"
  },
  {
    "name": "GPT-Neo 1.3B",
    "series": "GPT-Neo",
    "organization_name": "EleutherAI",
    "publication_name": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
    "publication_link": "http://github.com/eleutherai/gpt-neo",
    "publication_date": "2021-03-21",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 1.3,
    "model_availability": "public",
    "model_link": "https://github.com/EleutherAI/gpt-neo",
    "training_compute": 64,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/EleutherAI/gpt-neo",
    "training_framework": "TensorFlow",
    "training_provider": "GCP",
    "training_hardware": "TPU",
    "dataset": "The Pile",
    "dataset_unique_tokens": 194,
    "dataset_training_tokens": 380,
    "dataset_training_size": 825,
    "dataset_availability": "public",
    "dataset_link": "https://pile.eleuther.ai/",
    "dataset_content": "English"
  },
  {
    "name": "GPT-Neo 2.7B",
    "series": "GPT-Neo",
    "organization_name": "EleutherAI",
    "publication_name": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
    "publication_link": "http://github.com/eleutherai/gpt-neo",
    "publication_date": "2021-03-21",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 2.7,
    "model_availability": "public",
    "model_link": "https://github.com/EleutherAI/gpt-neo",
    "training_compute": 79,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/EleutherAI/gpt-neo",
    "training_framework": "TensorFlow",
    "training_provider": "GCP",
    "training_hardware": "TPU",
    "dataset": "The Pile",
    "dataset_unique_tokens": 194,
    "dataset_training_tokens": 420,
    "dataset_training_size": 825,
    "dataset_availability": "public",
    "dataset_link": "https://pile.eleuther.ai/",
    "dataset_content": "English"
  },
  {
    "name": "Megatron-LM",
    "series": "Megatron",
    "organization_name": "NVIDIA",
    "publication_name": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "publication_link": "https://arxiv.org/abs/1909.08053",
    "publication_date": "2019-09-17",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 8.3,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 91,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/NVIDIA/Megatron-LM",
    "training_framework": "PyTorch",
    "training_provider": "NVIDIA",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": "41?",
    "dataset_training_tokens": 157,
    "dataset_training_size": 174,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "Megatron-LM 2.5B",
    "series": "Megatron",
    "organization_name": "NVIDIA",
    "publication_name": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "publication_link": "https://arxiv.org/abs/1909.08053",
    "publication_date": "2019-09-17",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 2.5,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 27,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/NVIDIA/Megatron-LM",
    "training_framework": "PyTorch",
    "training_provider": "NVIDIA",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": "41?",
    "dataset_training_tokens": 157,
    "dataset_training_size": 174,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "Megatron-BERT 1.3B",
    "series": "Megatron",
    "organization_name": "NVIDIA",
    "publication_name": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "publication_link": "https://arxiv.org/abs/1909.08053",
    "publication_date": "2019-09-17",
    "model_type": "encoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 1.3,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 189,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/NVIDIA/Megatron-LM",
    "training_framework": "PyTorch",
    "training_provider": "NVIDIA",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": "41?",
    "dataset_training_tokens": 2097,
    "dataset_training_size": 174,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "Megatron-BERT",
    "series": "Megatron",
    "organization_name": "NVIDIA",
    "publication_name": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "publication_link": "https://arxiv.org/abs/1909.08053",
    "publication_date": "2019-09-17",
    "model_type": "encoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 3.9,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 568,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/NVIDIA/Megatron-LM",
    "training_framework": "PyTorch",
    "training_provider": "NVIDIA",
    "training_hardware": "V100",
    "dataset": "custom",
    "dataset_unique_tokens": "41?",
    "dataset_training_tokens": 2097,
    "dataset_training_size": 174,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "T5-3B",
    "series": "T5",
    "organization_name": "Google",
    "publication_name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "publication_link": "https://arxiv.org/abs/1910.10683",
    "publication_date": "2019-09-23",
    "model_type": "seq2seq",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 3,
    "model_availability": "public",
    "model_link": "https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints",
    "training_compute": 104,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/google-research/text-to-text-transfer-transformer",
    "training_framework": "TensorFlow",
    "training_provider": "GCP",
    "training_hardware": "TPU",
    "dataset": "C4",
    "dataset_unique_tokens": 177,
    "dataset_training_tokens": 1000,
    "dataset_training_size": 750,
    "dataset_availability": "public",
    "dataset_link": "https://www.tensorflow.org/datasets/catalog/c4",
    "dataset_content": "English"
  },
  {
    "name": "T5-11B",
    "series": "T5",
    "organization_name": "Google",
    "publication_name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "publication_link": "https://arxiv.org/abs/1910.10683",
    "publication_date": "2019-09-23",
    "model_type": "seq2seq",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 11,
    "model_availability": "public",
    "model_link": "https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints",
    "training_compute": 382,
    "training_code_availability": "public",
    "training_code_link": "https://github.com/google-research/text-to-text-transfer-transformer",
    "training_framework": "TensorFlow",
    "training_provider": "GCP",
    "training_hardware": "TPU",
    "dataset": "C4",
    "dataset_unique_tokens": 177,
    "dataset_training_tokens": 1000,
    "dataset_training_size": 750,
    "dataset_availability": "public",
    "dataset_link": "https://www.tensorflow.org/datasets/catalog/c4",
    "dataset_content": "English"
  },
  {
    "name": "Turing-NLG",
    "series": "Turing",
    "organization_name": "Microsoft",
    "publication_name": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
    "publication_link": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
    "publication_date": "2020-02-13",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "NLP",
    "model_parameters": 17,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 157,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch, DeepSpeed",
    "training_provider": "V100",
    "training_hardware": "Microsoft",
    "dataset": "custom",
    "dataset_unique_tokens": 177,
    "dataset_training_tokens": 1000,
    "dataset_training_size": 750,
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "English"
  },
  {
    "name": "iGPT-XL",
    "series": "iGPT",
    "organization_name": "OpenAI",
    "publication_name": "Generative Pretraining from Pixels",
    "publication_link": "http://proceedings.mlr.press/v119/chen20s.html",
    "publication_date": "2020-06-17",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "CV",
    "model_parameters": 6.8,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 248,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "TensorFlow",
    "training_provider": "TPU",
    "training_hardware": "GCP",
    "dataset": "web images",
    "dataset_unique_tokens": "?",
    "dataset_training_tokens": "128M images",
    "dataset_training_size": "?",
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "Images"
  },
  {
    "name": "iGPT-L",
    "series": "iGPT",
    "organization_name": "OpenAI",
    "publication_name": "Generative Pretraining from Pixels",
    "publication_link": "http://proceedings.mlr.press/v119/chen20s.html",
    "publication_date": "2020-06-17",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "CV",
    "model_parameters": 1.4,
    "model_availability": "public",
    "model_link": "",
    "training_compute": 248,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "TensorFlow",
    "training_provider": "TPU",
    "training_hardware": "GCP",
    "dataset": "web images",
    "dataset_unique_tokens": "?",
    "dataset_training_tokens": "128M images",
    "dataset_training_size": "?",
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": "Images"
  },
  {
    "name": "DALL-E",
    "series": "DALL-E",
    "organization_name": "OpenAI",
    "publication_name": "Zero-Shot Text-to-Image Generation",
    "publication_link": "https://arxiv.org/abs/2102.12092",
    "publication_date": "2020-02-24",
    "model_type": "decoder",
    "model_details": "",
    "model_modality": "multimodal",
    "model_parameters": 12,
    "model_availability": "not available",
    "model_link": "",
    "training_compute": 470,
    "training_code_availability": "not available",
    "training_code_link": "",
    "training_framework": "PyTorch",
    "training_provider": "Owl",
    "training_hardware": "V100",
    "dataset": "web images + captions",
    "dataset_unique_tokens": "250M images",
    "dataset_training_tokens": "440M images",
    "dataset_training_size": "?",
    "dataset_availability": "not available",
    "dataset_link": "",
    "dataset_content": ""
  }
]
